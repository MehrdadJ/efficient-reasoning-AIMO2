{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f63211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:53:04.921813Z",
     "iopub.status.busy": "2025-03-27T21:53:04.921565Z",
     "iopub.status.idle": "2025-03-27T21:58:50.785225Z",
     "shell.execute_reply": "2025-03-27T21:58:50.784431Z"
    },
    "papermill": {
     "duration": 345.869786,
     "end_time": "2025-03-27T21:58:50.786735",
     "exception": false,
     "start_time": "2025-03-27T21:53:04.916949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import ast\n",
    "import math\n",
    "import kaggle_evaluation.aimo_2_inference_server\n",
    "from vllm import LLM, SamplingParams\n",
    "import vllm\n",
    "\n",
    "print(vllm.__version__)\n",
    "\n",
    "SEED = 42\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=SEED)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "start_time = time.time()\n",
    "\n",
    "# Adjust these for your environment / runtime limits\n",
    "cutoff_time = start_time + (4 * 60 + 45) * 60\n",
    "cutoff_times = [int(x) for x in np.linspace(cutoff_time, start_time + 60 * 60, 50 + 1)]\n",
    "\n",
    "# llm_model_pth = '/kaggle/input/deepseek-r1/transformers/deepseek-aideepseek-r1-distill-qwen-14b-awq-neody/1'\n",
    "llm_model_pth = '/kaggle/input/scale7b-alpha0.1/transformers/14b-alpha0.2/1'\n",
    "MAX_NUM_SEQS = 3\n",
    "MAX_MODEL_LEN = 16384\n",
    "\n",
    "llm = LLM(\n",
    "    llm_model_pth,\n",
    "    dtype=\"half\", \n",
    "    max_num_seqs=MAX_NUM_SEQS,\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    trust_remote_code=True,\n",
    "    tensor_parallel_size=4,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    seed=SEED,\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66947b67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:58:50.805915Z",
     "iopub.status.busy": "2025-03-27T21:58:50.805639Z",
     "iopub.status.idle": "2025-03-27T21:58:50.810045Z",
     "shell.execute_reply": "2025-03-27T21:58:50.809442Z"
    },
    "papermill": {
     "duration": 0.014996,
     "end_time": "2025-03-27T21:58:50.811072",
     "exception": false,
     "start_time": "2025-03-27T21:58:50.796076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_boxed_text(text):\n",
    "    # Try to find answers in \\boxed{...} format first\n",
    "    boxed_pattern = r'\\\\boxed{(.*?)}'\n",
    "    boxed_matches = re.findall(boxed_pattern, text)\n",
    "    for match in boxed_matches[::-1]:\n",
    "        if match.strip() != \"\":\n",
    "            return match.strip()\n",
    "    \n",
    "    # Look for \"the answer is X\" or similar patterns\n",
    "    answer_patterns = [\n",
    "        r'the answer is[:\\s]+([0-9]+)',\n",
    "        r'answer[:\\s=]+([0-9]+)',\n",
    "        r'final answer[:\\s=]+([0-9]+)',\n",
    "        r'answer is[:\\s]+([0-9]+)',\n",
    "        r'the answer is\\s*([0-9]+)'  # \\s* means zero or more whitespace characters\n",
    "        r'answer is\\s*([0-9]+)',\n",
    "        r'therefore.*?is\\s*([0-9]+)'\n",
    "        # r'therefore,? the answer is[:\\s]+([0-9]+)',\n",
    "        # r'we get[:\\s]+([0-9]+)',\n",
    "        # r'result is[:\\s]+([0-9]+)',\n",
    "        # r'the result is[:\\s]+([0-9]+)',\n",
    "        # r'value is[:\\s]+([0-9]+)',\n",
    "        # r'the value is[:\\s]+([0-9]+)',\n",
    "        # r'our answer is[:\\s]+([0-9]+)',\n",
    "        # r'equals[:\\s]+([0-9]+)$',\n",
    "        # r'=\\s*([0-9]+)$',\n",
    "        # r'gives us[:\\s]+([0-9]+)',\n",
    "        # # Match statements at the end of text that are just numbers\n",
    "        # r'(?:^|\\n)\\s*([0-9]+)\\s*$'\n",
    "    ]\n",
    "    \n",
    "    for pattern in answer_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        if matches:\n",
    "            return matches[-1].strip()\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "165834d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:58:50.829685Z",
     "iopub.status.busy": "2025-03-27T21:58:50.829457Z",
     "iopub.status.idle": "2025-03-27T21:58:50.834526Z",
     "shell.execute_reply": "2025-03-27T21:58:50.833938Z"
    },
    "papermill": {
     "duration": 0.015286,
     "end_time": "2025-03-27T21:58:50.835457",
     "exception": false,
     "start_time": "2025-03-27T21:58:50.820171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_answer(answers):\n",
    "    if not answers:\n",
    "        return 210\n",
    "        \n",
    "    # Try to convert all answers to integers\n",
    "    valid_answers = []\n",
    "    for answer in answers:\n",
    "        try:\n",
    "            answer_int = int(answer)\n",
    "            if answer_int == float(answer):  # Ensure it's a valid integer\n",
    "                valid_answers.append(answer_int)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if not valid_answers:\n",
    "        return 210\n",
    "    \n",
    "    # Count frequencies\n",
    "    counter = Counter(valid_answers)\n",
    "    \n",
    "    # Get the most common answer\n",
    "    most_common = counter.most_common()\n",
    "    \n",
    "    # If there's a clear winner with multiple votes, use it\n",
    "    if len(most_common) > 1 and most_common[0][1] > most_common[1][1]:\n",
    "        return most_common[0][0] % 1000\n",
    "        \n",
    "    # For digits sum problems specifically (common pattern in problem IDs)\n",
    "    if any(\"sum of digit\" in str(answer).lower() for answer in answers):\n",
    "        # These problems often have answers ending in 9 or 1\n",
    "        nine_answers = [a for a in valid_answers if a % 10 == 9]\n",
    "        if nine_answers:\n",
    "            return nine_answers[0] % 1000\n",
    "    \n",
    "    # Otherwise use the most common answer\n",
    "    return most_common[0][0] % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec00b95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:58:50.853842Z",
     "iopub.status.busy": "2025-03-27T21:58:50.853622Z",
     "iopub.status.idle": "2025-03-27T21:58:50.859516Z",
     "shell.execute_reply": "2025-03-27T21:58:50.858938Z"
    },
    "papermill": {
     "duration": 0.01617,
     "end_time": "2025-03-27T21:58:50.860500",
     "exception": false,
     "start_time": "2025-03-27T21:58:50.844330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_starter_messages(question, index):\n",
    "    # For Scale7B, we can use a simple text format instead of trying to use the chat template\n",
    "    # This matches the fine-tuning input format exactly\n",
    "    prompt = f'<｜begin▁of▁sentence｜><｜User｜>Please reason step by step, and put your final answer within \\\\boxed{{}} after taking modulo 1000. Question: {question}<｜Assistant｜>'\n",
    "    \n",
    "    options = []\n",
    "    for _ in range(13):\n",
    "        options.append([prompt])  # Return just the prompt string\n",
    "    \n",
    "    return options[index%len(options)]\n",
    "\n",
    "def batch_message_generate(list_of_messages) -> list[list[dict]]:\n",
    "    max_tokens = MAX_MODEL_LEN\n",
    "    if time.time() > cutoff_times[-1]:\n",
    "        print(\"Speedrun\")\n",
    "        max_tokens = 2 * MAX_MODEL_LEN // 3\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "        min_p=0.01,\n",
    "        seed=SEED,\n",
    "        skip_special_tokens=True,\n",
    "        max_tokens=max_tokens,\n",
    "        stop=[\"</think>\"]\n",
    "    )\n",
    "    \n",
    "    # No need to apply chat template - use the prompts directly\n",
    "    list_of_texts = [messages[0] for messages in list_of_messages]  # Extract the prompt string\n",
    "\n",
    "    request_output = llm.generate(\n",
    "        prompts=list_of_texts,\n",
    "        sampling_params=sampling_params,\n",
    "    )\n",
    "    \n",
    "    # Process the outputs as before\n",
    "    sort_keys_and_list_of_messages = []\n",
    "\n",
    "    for messages, single_request_output in zip(list_of_messages, request_output):\n",
    "        messages.append({'role': 'assistant', 'content': single_request_output.outputs[0].text})\n",
    "\n",
    "        sort_keys_and_list_of_messages.append(\n",
    "            (\n",
    "                len(single_request_output.outputs[0].token_ids),\n",
    "                messages\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sort_keys_and_list_of_messages.sort(key=lambda sort_key_and_messages: sort_key_and_messages[0])\n",
    "    list_of_messages = [messages for _, messages in sort_keys_and_list_of_messages]\n",
    "    \n",
    "    return list_of_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b665db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:58:50.878803Z",
     "iopub.status.busy": "2025-03-27T21:58:50.878581Z",
     "iopub.status.idle": "2025-03-27T21:58:50.882627Z",
     "shell.execute_reply": "2025-03-27T21:58:50.882054Z"
    },
    "papermill": {
     "duration": 0.014183,
     "end_time": "2025-03-27T21:58:50.883643",
     "exception": false,
     "start_time": "2025-03-27T21:58:50.869460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_message_filter(list_of_messages) -> tuple[list[list[dict]], list[str]]:\n",
    "    extracted_answers = []\n",
    "    list_of_messages_to_keep = []\n",
    "    \n",
    "    print(\"\\n=== CoT Token Counts and Answers ===\")\n",
    "    for i, messages in enumerate(list_of_messages):\n",
    "        # Get token count from the most recent response\n",
    "        # This uses the actual token count from the tokenizer\n",
    "        token_count = len(tokenizer.encode(messages[-1]['content']))\n",
    "        \n",
    "        # Extract answer\n",
    "        answer = extract_boxed_text(messages[-1]['content'])\n",
    "        \n",
    "        # Print the information\n",
    "        print(f\"CoT{i+1}: {token_count} tokens, answer: {answer if answer else 'None'}\")\n",
    "        \n",
    "        if answer:\n",
    "            extracted_answers.append(answer)\n",
    "        else:\n",
    "            list_of_messages_to_keep.append(messages)\n",
    "    \n",
    "    return list_of_messages_to_keep, extracted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c79d6fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:58:50.901917Z",
     "iopub.status.busy": "2025-03-27T21:58:50.901675Z",
     "iopub.status.idle": "2025-03-27T21:58:50.907088Z",
     "shell.execute_reply": "2025-03-27T21:58:50.906507Z"
    },
    "papermill": {
     "duration": 0.015624,
     "end_time": "2025-03-27T21:58:50.908087",
     "exception": false,
     "start_time": "2025-03-27T21:58:50.892463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_for_question(question: str) -> int:\n",
    "\n",
    "    if time.time() > cutoff_time:\n",
    "        return 210\n",
    "    \n",
    "    print(question)\n",
    "\n",
    "    num_seqs = MAX_NUM_SEQS\n",
    "    if time.time() > cutoff_times[-1]:\n",
    "        num_seqs = 2 * MAX_NUM_SEQS // 3\n",
    "    \n",
    "    list_of_messages = [create_starter_messages(question, index) for index in range(num_seqs)]\n",
    "    original_list = list_of_messages.copy()  # Save a copy of the original list\n",
    "    \n",
    "    all_extracted_answers = []\n",
    "    for _ in range(1):\n",
    "        list_of_messages = batch_message_generate(list_of_messages)\n",
    "        \n",
    "        # Save debug output\n",
    "        if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                    \"question\": [question] * len(list_of_messages),\n",
    "                    \"message\": [messages[-1][\"content\"] for messages in list_of_messages],\n",
    "                }\n",
    "            )\n",
    "            df.to_csv(f\"{str(int(time.time() - start_time)).zfill(5)}.csv\", index=False)\n",
    "        \n",
    "        # Original processing\n",
    "        list_of_messages, extracted_answers = batch_message_filter(list_of_messages)\n",
    "        all_extracted_answers.extend(extracted_answers)\n",
    "    \n",
    "    # Choose between majority voting and shortest CoT selection\n",
    "    use_shortest_cot = False  # Set to True to use shortest CoT selection\n",
    "    \n",
    "    if use_shortest_cot:\n",
    "        answer = select_shortest_cot_answer(list_of_messages, all_extracted_answers)\n",
    "    else:\n",
    "        answer = select_answer(all_extracted_answers)\n",
    "    \n",
    "    print(f\"Final answer: {answer}\")\n",
    "    print(\"\\n\\n\")\n",
    "    cutoff_times.pop()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0826b02b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:58:50.926271Z",
     "iopub.status.busy": "2025-03-27T21:58:50.926057Z",
     "iopub.status.idle": "2025-03-27T21:58:50.929391Z",
     "shell.execute_reply": "2025-03-27T21:58:50.928794Z"
    },
    "papermill": {
     "duration": 0.013516,
     "end_time": "2025-03-27T21:58:50.930380",
     "exception": false,
     "start_time": "2025-03-27T21:58:50.916864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    \n",
    "    question = question.item(0)\n",
    "    answer = predict_for_question(question)\n",
    "    # print(question)\n",
    "    # print(\"------\\n\\n\\n\")\n",
    "    return pl.DataFrame({'id': id_, 'answer': answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee0a6812",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:58:50.948845Z",
     "iopub.status.busy": "2025-03-27T21:58:50.948628Z",
     "iopub.status.idle": "2025-03-27T21:58:50.972507Z",
     "shell.execute_reply": "2025-03-27T21:58:50.971908Z"
    },
    "papermill": {
     "duration": 0.034164,
     "end_time": "2025-03-27T21:58:50.973566",
     "exception": false,
     "start_time": "2025-03-27T21:58:50.939402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26840c26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:58:50.991936Z",
     "iopub.status.busy": "2025-03-27T21:58:50.991694Z",
     "iopub.status.idle": "2025-03-27T21:58:50.994139Z",
     "shell.execute_reply": "2025-03-27T21:58:50.993558Z"
    },
    "papermill": {
     "duration": 0.012701,
     "end_time": "2025-03-27T21:58:50.995138",
     "exception": false,
     "start_time": "2025-03-27T21:58:50.982437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "\n",
    "# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#     inference_server.serve()\n",
    "# else:\n",
    "#     inference_server.run_local_gateway(\n",
    "#         (\n",
    "#             'reference.csv',\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d462643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T21:58:51.013479Z",
     "iopub.status.busy": "2025-03-27T21:58:51.013261Z",
     "iopub.status.idle": "2025-03-27T21:58:51.016637Z",
     "shell.execute_reply": "2025-03-27T21:58:51.016043Z"
    },
    "papermill": {
     "duration": 0.01359,
     "end_time": "2025-03-27T21:58:51.017662",
     "exception": false,
     "start_time": "2025-03-27T21:58:51.004072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in competition environment - Saved!\n"
     ]
    }
   ],
   "source": [
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    print(\"Not in competition environment - Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11376393,
     "isSourceIdPinned": false,
     "sourceId": 86023,
     "sourceType": "competition"
    },
    {
     "datasetId": 6924235,
     "sourceId": 11106696,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6925853,
     "sourceId": 11108984,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 220483900,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 244643,
     "modelInstanceId": 222884,
     "sourceId": 260716,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 244643,
     "modelInstanceId": 228856,
     "sourceId": 267427,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 224053,
     "modelInstanceId": 202302,
     "sourceId": 236869,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 244643,
     "modelInstanceId": 237635,
     "sourceId": 277462,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 270770,
     "modelInstanceId": 249253,
     "sourceId": 290910,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 225262,
     "modelInstanceId": 204046,
     "sourceId": 256586,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 244643,
     "modelInstanceId": 241178,
     "sourceId": 281476,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 354.973208,
   "end_time": "2025-03-27T21:58:56.182827",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-27T21:53:01.209619",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "29596440c933443e8368805405e092ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3da8ca9da8e248d9b8de20559d009327": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4a072a305bcc456eaed450abea683ad9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "58a9bdcffb1d4ef09eefb5329178ee8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_894b8d5b0ff64f62b20da59c92fff437",
        "IPY_MODEL_8d1582497ca6425da0cbc7051ba12a20",
        "IPY_MODEL_5fedf07d96034673961d95ab2a640f28"
       ],
       "layout": "IPY_MODEL_827ac0e6ac8542738b3201e95aa41557",
       "tabbable": null,
       "tooltip": null
      }
     },
     "5fedf07d96034673961d95ab2a640f28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4a072a305bcc456eaed450abea683ad9",
       "placeholder": "​",
       "style": "IPY_MODEL_75c53184f1a148e99f89ff6f99e2237f",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading safetensors checkpoint shards: 100% Completed | 6/6 [03:02&lt;00:00, 31.34s/it]\n"
      }
     },
     "75c53184f1a148e99f89ff6f99e2237f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "827ac0e6ac8542738b3201e95aa41557": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "86d58d6dae2b428cab264ed3c45ccddf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "894b8d5b0ff64f62b20da59c92fff437": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_29596440c933443e8368805405e092ba",
       "placeholder": "​",
       "style": "IPY_MODEL_3da8ca9da8e248d9b8de20559d009327",
       "tabbable": null,
       "tooltip": null,
       "value": ""
      }
     },
     "8d1582497ca6425da0cbc7051ba12a20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ce87a106070048bb9e05ee3b4682bb65",
       "max": 6,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_86d58d6dae2b428cab264ed3c45ccddf",
       "tabbable": null,
       "tooltip": null,
       "value": 6
      }
     },
     "ce87a106070048bb9e05ee3b4682bb65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
